{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAMP: Picture reconstruction\n",
    "*Alexia Allal, Gabriel Buffet, Stéphane Dumas, Nicolas Giraudet de Boudemange, Sébastien Mandela Yvon, Jérémy Pinault from IPP/M2DS*\n",
    "\n",
    "AJOUTER LOGO IPP\n",
    "<!-- <div>\n",
    "<table style=\"width:100%; background-color:transparent;\">\n",
    "  <tr style=\"background-color:transparent;\">\n",
    "    <td align=\"left\"; style=\"background-color:transparent; width: 40%;\">\n",
    "        <a href=\"https://dataia.eu\">\n",
    "            <img src=\"https://github.com/ramp-kits/tephra/raw/main/img/DATAIA-h.png\" width=\"450px\"/>\n",
    "        </a>\n",
    "    </td>\n",
    "    <td align=\"right\"; style=\"background-color:transparent; width: 40%;\">\n",
    "        <a href=\"https://www.geops.universite-paris-saclay.fr/\">\n",
    "            <img src=\"https://github.com/ramp-kits/tephra/raw/main/img/LOGO-GEOPS-2020-1024x488-1.jpg\" width=\"350px\"/>\n",
    "        </a>\n",
    "    </td>\n",
    "  </tr>\n",
    " </table>\n",
    "</div> -->\n",
    "\n",
    "\n",
    "CHANGER TABLE DES MATIERES\n",
    "## Table of Contents\n",
    "* [Introduction](#introduction)\n",
    "* [The dataset](#dataset)\n",
    "* [Data exploration](#exploration)\n",
    "* [Requirements](#requirements)\n",
    "* [Base model](#base_model)\n",
    "* [Submitting on RAMP](#submitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "The Unsplash Dataset is made up of over 350,000+ contributing global photographers and data sourced from hundreds of millions of searches across a nearly unlimited number of uses and contexts. Due to the breadth of intent and semantics contained within the Unsplash dataset, it enables new opportunities for research and learning.\n",
    "\n",
    "The Unsplash Dataset is offered in two datasets:\n",
    "\n",
    "* the Lite dataset: available for commercial and noncommercial usage, containing 25k nature-themed Unsplash photos, 25k keywords, and 1M searches\n",
    "* the Full dataset: available for noncommercial usage, containing 5.4M+ high-quality Unsplash photos, 5M keywords, and over 250M searches\n",
    "\n",
    "As the Unsplash library continues to grow, we’ll release updates to the dataset with new fields and new images, with each subsequent release being [semantically versioned](https://semver.org/).\n",
    "\n",
    "We welcome any feedback regarding the content of the datasets or their format. With your input, we hope to close the gap between the data we provide and the data that you would like to leverage. You can [open an issue](https://github.com/unsplash/datasets/issues/new/choose) to report a problem or to let us know what you would like to see in the next release of the datasets.\n",
    "\n",
    "For more on the Unsplash Dataset, see the [announcement](https://unsplash.com/blog/the-unsplash-dataset/) and [site](https://unsplash.com/data).\n",
    "\n",
    "To create this challenge, we use the Lite dataset and we downgrade the quality of the images.\n",
    "\n",
    "The goal of our challenge is to reconstruct the original pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset <a class=\"anchor\" id=\"dataset\"></a>\n",
    "\n",
    "\n",
    "In the Dataset, you will find the original picture (the ground truth) that you must to reconstruct with a low resolution version of this image.\n",
    "\n",
    "The low resolution image has been obtained with a simple downsampling by a factor of 5 in each spatial dimension and conversion to black & white.\n",
    "\n",
    "For example, if the original picture has a shape of (800, 640, 3) (Height, Width, Channels), you will be provided with a low resolution version of the orginial picure with a shape (160, 128).\n",
    "\n",
    "The Public Dataset you get access contains 15 000 pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a name=Setup></a>\n",
    "### Prerequisites\n",
    "The following cell will install the required package dependencies, if necessary. You can examine the file, `requirements.txt`, included in the repo to view the list of dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: ramp in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.1.4)\n",
      "Requirement already satisfied: wget in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (3.2)\n",
      "Collecting gdown (from -r requirements.txt (line 4))\n",
      "  Obtaining dependency information for gdown from https://files.pythonhosted.org/packages/cb/56/f4845ed78723a4eb8eb22bcfcb46e1157a462c78c0a5ed318c68c98f9a79/gdown-5.1.0-py3-none-any.whl.metadata\n",
      "  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from gdown->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: filelock in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from gdown->-r requirements.txt (line 4)) (3.9.0)\n",
      "Requirement already satisfied: requests[socks] in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from gdown->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from gdown->-r requirements.txt (line 4)) (4.65.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 4)) (2.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (2023.11.17)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/sdumas/anaconda3/lib/python3.11/site-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (1.7.1)\n",
      "Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-5.1.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration <a name=data_exp></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download\n",
    "\n",
    "Data are avaialable on a public drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 400: Bad Request",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Uncomment the following line to download data:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdownload_data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m download_data\u001b[38;5;241m.\u001b[39mdata_fetch()\n",
      "File \u001b[0;32m~/Documents/MASTER_DS/Data_Camp/ZZ_Data_challenge_group/Git/picture_reconstruction/download_data.py:46\u001b[0m, in \u001b[0;36mdata_fetch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_fetch\u001b[39m():\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     wget\u001b[38;5;241m.\u001b[39mdownload(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl_train\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     47\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl_train\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     48\u001b[0m     wget\u001b[38;5;241m.\u001b[39mdownload(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl_test\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/wget.py:526\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(url, out, bar)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     binurl \u001b[38;5;241m=\u001b[39m url\n\u001b[0;32m--> 526\u001b[0m (tmpfile, headers) \u001b[38;5;241m=\u001b[39m ulib\u001b[38;5;241m.\u001b[39murlretrieve(binurl, tmpfile, callback)\n\u001b[1;32m    527\u001b[0m filename \u001b[38;5;241m=\u001b[39m detect_filename(url, out, headers)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outdir:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:241\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(urlopen(url, data)) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    242\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m meth(req, response)\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, request, response, code, msg, hdrs)\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request"
     ]
    }
   ],
   "source": [
    "# Uncomment the following line to download data:\n",
    "import download_data\n",
    "download_data.data_fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass, subprocess\n",
    "## Decrypt the data.\n",
    "subprocess.call(['tar', '-xzf', 'Train.tgz'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data <a name=visualize></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration <a class=\"anchor\" id=\"exploration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(...\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(...\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model <a class=\"anchor\" id=\"base_model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATION DES X_TRAIN ET Y_TRAIN, ET X_TEST ET Y_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = ..\n",
    "Y_train_df = .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df = ..\n",
    "Y_test_df = .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation and scaling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier: here we take a simple model, logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(\n",
    "    steps=[(\"transformer\", numeric_transformer), (\"classifier\", LogisticRegression(max_iter=500))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the pipeline on train data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy score and balanced accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "fig = disp.figure_\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balanced accuracy score will be used as the leaderboard score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the score to beat in this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to the online challenge: [ramp.studio](https://ramp.studio) <a class=\"anchor\" id=\"submitting\"></a>\n",
    "\n",
    "Once you found a good model, you can submit them to [ramp.studio](https://www.ramp.studio) to enter the online challenge. First, if it is your first time using the RAMP platform, [sign up](https://www.ramp.studio/sign_up), otherwise [log in](https://www.ramp.studio/login). Then sign up to the event [tephra](http://www.ramp.studio/events/tephra_datacamp2023). Both signups are controled by RAMP administrators, so there **can be a delay between asking for signup and being able to submit**.\n",
    "\n",
    "Once your signup request is accepted, you can go to your [sandbox](https://www.ramp.studio/events/tephra_datacamp2023/sandbox) and write the code for your classifier directly on the browser. You can also create a new folder `my_submission` in the `submissions` folder containing `classifier.py` and upload this file directly. You can check the starting-kit ([`classifier.py`](/edit/submissions/starting_kit/classifier.py)) for an example. The submission is trained and tested on our backend in the similar way as `ramp-test` does it locally. While your submission is waiting in the queue and being trained, you can find it in the \"New submissions (pending training)\" table in [my submissions](https://www.ramp.studio/events/tephra_datacamp2023/my_submissions). Once it is trained, your submission shows up on the [public leaderboard](https://www.ramp.studio/events/tephra_datacamp2023/leaderboard). \n",
    "If there is an error (despite having tested your submission locally with `ramp-test`), it will show up in the \"Failed submissions\" table in [my submissions](https://www.ramp.studio/events/tephra_datacamp2023/my_submissions). You can click on the error to see part of the trace.\n",
    "\n",
    "The data set we use at the backend is usually different from what you find in the starting kit, so the score may be different.\n",
    "\n",
    "The usual way to work with RAMP is to explore solutions, add feature transformations, select models, etc., _locally_, and checking them with `ramp-test`. The script prints mean cross-validation scores.\n",
    "\n",
    "The official score in this RAMP (the first score column on the [leaderboard](http://www.ramp.studio/events/tephra_datacamp2023/leaderboard)) is the balenced accuracy score (`bal_acc`). When the score is good enough, you can submit it at the RAMP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the script proposed as the starting_kit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.transformer = Pipeline(\n",
    "            steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler()),\n",
    "            ]\n",
    "        )\n",
    "        self.model = LogisticRegression(max_iter=500)\n",
    "        self.pipe = make_pipeline(self.transformer, self.model)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.drop([\"groups\"], axis=1)\n",
    "        self.pipe.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.drop([\"groups\"], axis=1)\n",
    "        return self.pipe.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = X.drop([\"groups\"], axis=1)\n",
    "        return self.pipe.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your solution locally by running the ramp-test command followed by --submission <my_submission folder>.\n",
    "Here is an example with the starting_kit submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: ramp-test\n"
     ]
    }
   ],
   "source": [
    "!ramp-test --submission starting_kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "See the [online documentation](https://paris-saclay-cds.github.io/ramp-docs/ramp-workflow/stable/using_kits.html) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Questions related to the starting kit should be asked on the issue tracker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ramp-tephra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
